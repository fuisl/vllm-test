services:
  vllm-embed:
    image: vllm/vllm-openai:latest
    # user: "${UID}:${GID}"               # make sure UID/GID are defined in your .env
    gpus: all
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}   # use this exact var name
      - XDG_CACHE_HOME=/home/fuisloy/.cache
      - HF_HOME=/home/fuisloy/.cache/huggingface
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    ports:
      - "8000:8000"
    volumes:
      - ${HOME}/.cache/hf-vllm:/home/fuisloy/.cache/huggingface
    ipc: host
    command: >
      --model BAAI/bge-m3
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization 0.25
    networks: [net]

  vllm-chat:
    image: vllm/vllm-openai:latest
    # user: "${UID}:${GID}"               # make sure UID/GID are defined in your .env
    gpus: all
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}   # use this exact var name
      - XDG_CACHE_HOME=/home/fuisloy/.cache
      - HF_HOME=/home/fuisloy/.cache/huggingface
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - VLLM_USE_TRITON_FLASH_ATTENTION=0
    ports:
      - "8001:8000"
    volumes:
      - ${HOME}/.cache/hf-vllm:/home/fuisloy/.cache/huggingface
    ipc: host
    command: >
      --model Qwen/Qwen2.5-7B-Instruct
      --host 0.0.0.0
      --port 8001
      --gpu-memory-utilization 0.93
    networks: [net]

  proxy:
    image: nginx:alpine
    ports: ["8002:80"]
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - vllm-embed
      - vllm-chat
    networks: [net]
  
networks:
  net: