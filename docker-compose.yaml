services:
  vllm-embed:
    image: vllm/vllm-openai:latest
    # user: "${UID}:${GID}" # for identify yourself in remote server
    gpus: all
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - XDG_CACHE_HOME=/home/fuisloy/.cache
      - HF_HOME=/home/fuisloy/.cache/huggingface
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    ports:
      - "8000:8000"
    volumes:
      - ${HOME}/.cache/hf-vllm:/home/fuisloy/.cache/huggingface
    ipc: host
    command: >
      --model BAAI/bge-m3
      --host 0.0.0.0
      --port 8000
      --gpu-memory-utilization 0.09

  vllm-chat:
    image: vllm/vllm-openai:latest
    # user: "${UID}:${GID}" 
    gpus: all
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - XDG_CACHE_HOME=/home/fuisloy/.cache
      - HF_HOME=/home/fuisloy/.cache/huggingface
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - VLLM_USE_TRITON_FLASH_ATTENTION=0
    ports:
      - "8001:8000"
    volumes:
      - ${HOME}/.cache/hf-vllm:/home/fuisloy/.cache/huggingface
    ipc: host
    command: >
      --model Qwen/Qwen2.5-7B-Instruct-AWQ
      --host 0.0.0.0
      --port 8001
      --gpu-memory-utilization 0.31

  proxy:
    image: nginx:alpine
    ports: ["8002:80"]
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro